{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8d26d8",
   "metadata": {
    "papermill": {
     "duration": 0.028162,
     "end_time": "2022-06-19T16:15:34.932930",
     "exception": false,
     "start_time": "2022-06-19T16:15:34.904768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Directory settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7084e81c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:15:34.996640Z",
     "iopub.status.busy": "2022-06-19T16:15:34.995155Z",
     "iopub.status.idle": "2022-06-19T16:15:35.004896Z",
     "shell.execute_reply": "2022-06-19T16:15:35.004327Z",
     "shell.execute_reply.started": "2022-06-19T15:41:45.400885Z"
    },
    "id": "fa3b873b",
    "papermill": {
     "duration": 0.042896,
     "end_time": "2022-06-19T16:15:35.005046",
     "exception": false,
     "start_time": "2022-06-19T16:15:34.962150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "INPUT_DIR = '../input/us-patent-phrase-to-phrase-matching/'\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d453e0",
   "metadata": {
    "id": "1d0c4430",
    "papermill": {
     "duration": 0.029226,
     "end_time": "2022-06-19T16:15:35.063737",
     "exception": false,
     "start_time": "2022-06-19T16:15:35.034511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa661b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:15:35.131406Z",
     "iopub.status.busy": "2022-06-19T16:15:35.130583Z",
     "iopub.status.idle": "2022-06-19T16:15:35.134397Z",
     "shell.execute_reply": "2022-06-19T16:15:35.134803Z",
     "shell.execute_reply.started": "2022-06-19T15:43:56.528551Z"
    },
    "id": "48dd82bb",
    "papermill": {
     "duration": 0.041704,
     "end_time": "2022-06-19T16:15:35.134943",
     "exception": false,
     "start_time": "2022-06-19T16:15:35.093239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CFG:\\n    num_workers=4\\n    path=\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"\\n    config_path=path+\\'config.pth\\'\\n    model=\"microsoft/deberta-v3-large\"\\n    batch_size=32\\n    fc_dropout=0.2\\n    target_size=1\\n    max_len=133\\n    seed=42\\n    n_fold=4\\n    trn_fold=[0, 1, 2, 3]\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "\"\"\"\n",
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=\"../input/pppm-deberta-v3-large-baseline-w-w-b-train/\"\n",
    "    config_path=path+'config.pth'\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    batch_size=32\n",
    "    fc_dropout=0.2\n",
    "    target_size=1\n",
    "    max_len=133\n",
    "    seed=42\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923ae0d",
   "metadata": {
    "id": "f2ed8ef2",
    "papermill": {
     "duration": 0.028744,
     "end_time": "2022-06-19T16:15:35.193528",
     "exception": false,
     "start_time": "2022-06-19T16:15:35.164784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "684637b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:15:35.266765Z",
     "iopub.status.busy": "2022-06-19T16:15:35.265911Z",
     "iopub.status.idle": "2022-06-19T16:16:03.396589Z",
     "shell.execute_reply": "2022-06-19T16:16:03.395771Z",
     "shell.execute_reply.started": "2022-06-19T15:43:57.552098Z"
    },
    "executionInfo": {
     "elapsed": 20123,
     "status": "ok",
     "timestamp": 1644920080956,
     "user": {
      "displayName": "Yasufumi Nakama",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "17486303986134302670"
     },
     "user_tz": -540
    },
    "id": "35916341",
    "outputId": "06fa0ab8-a380-4f54-a98d-b7015b79d9e2",
    "papermill": {
     "duration": 28.173427,
     "end_time": "2022-06-19T16:16:03.396724",
     "exception": false,
     "start_time": "2022-06-19T16:15:35.223297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.9.1\n",
      "Found existing installation: transformers 4.16.2\n",
      "Uninstalling transformers-4.16.2:\n",
      "  Successfully uninstalled transformers-4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tokenizers 0.11.6\n",
      "Uninstalling tokenizers-0.11.6:\n",
      "  Successfully uninstalled tokenizers-0.11.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/pppm-pip-wheels-dataset\n",
      "Processing /kaggle/input/pppm-pip-wheels-dataset/transformers-4.16.2-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.49)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Processing /kaggle/input/pppm-pip-wheels-dataset/tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "Successfully installed tokenizers-0.11.6 transformers-4.16.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: ../input/pppm-pip-wheels-dataset\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.11.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.11.6\n",
      "transformers.__version__: 4.16.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "print(f\"torch.__version__: {torch.__version__}\")\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "os.system('pip uninstall -y transformers')\n",
    "os.system('pip uninstall -y tokenizers')\n",
    "os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset transformers')\n",
    "os.system('python -m pip install --no-index --find-links=../input/pppm-pip-wheels-dataset tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb987a7",
   "metadata": {
    "id": "fd586614",
    "papermill": {
     "duration": 0.033836,
     "end_time": "2022-06-19T16:16:03.465787",
     "exception": false,
     "start_time": "2022-06-19T16:16:03.431951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56e5432",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:16:03.538933Z",
     "iopub.status.busy": "2022-06-19T16:16:03.538093Z",
     "iopub.status.idle": "2022-06-19T16:16:03.539941Z",
     "shell.execute_reply": "2022-06-19T16:16:03.540333Z",
     "shell.execute_reply.started": "2022-06-19T15:44:32.099007Z"
    },
    "papermill": {
     "duration": 0.040729,
     "end_time": "2022-06-19T16:16:03.540490",
     "exception": false,
     "start_time": "2022-06-19T16:16:03.499761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d32acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:16:03.789193Z",
     "iopub.status.busy": "2022-06-19T16:16:03.788311Z",
     "iopub.status.idle": "2022-06-19T16:18:09.814844Z",
     "shell.execute_reply": "2022-06-19T16:18:09.814341Z",
     "shell.execute_reply.started": "2022-06-19T15:44:32.242228Z"
    },
    "papermill": {
     "duration": 126.065143,
     "end_time": "2022-06-19T16:18:09.814984",
     "exception": false,
     "start_time": "2022-06-19T16:16:03.749841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifer.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:06<00:00,  2.28s/it]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifer.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.50it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifer.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.40it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifer.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.20it/s]\n",
      "Some weights of the model checkpoint at ../input/deberta-v3-large/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifer.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifer.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code1.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e37473de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:18:10.104296Z",
     "iopub.status.busy": "2022-06-19T16:18:10.103680Z",
     "iopub.status.idle": "2022-06-19T16:20:11.117385Z",
     "shell.execute_reply": "2022-06-19T16:20:11.117864Z",
     "shell.execute_reply.started": "2022-06-19T15:46:43.888183Z"
    },
    "papermill": {
     "duration": 121.061916,
     "end_time": "2022-06-19T16:20:11.118038",
     "exception": false,
     "start_time": "2022-06-19T16:18:10.056122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.20s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.71it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.66it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.50it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code4.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32d970ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:20:11.453186Z",
     "iopub.status.busy": "2022-06-19T16:20:11.452581Z",
     "iopub.status.idle": "2022-06-19T16:21:51.428529Z",
     "shell.execute_reply": "2022-06-19T16:21:51.427800Z",
     "shell.execute_reply.started": "2022-06-19T15:48:47.799638Z"
    },
    "papermill": {
     "duration": 100.03085,
     "end_time": "2022-06-19T16:21:51.428720",
     "exception": false,
     "start_time": "2022-06-19T16:20:11.397870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.68s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.17s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.20s/it]\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code5.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99260d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:21:52.039240Z",
     "iopub.status.busy": "2022-06-19T16:21:52.038395Z",
     "iopub.status.idle": "2022-06-19T16:24:48.169560Z",
     "shell.execute_reply": "2022-06-19T16:24:48.169993Z",
     "shell.execute_reply.started": "2022-06-19T15:50:26.690149Z"
    },
    "papermill": {
     "duration": 176.206341,
     "end_time": "2022-06-19T16:24:48.170179",
     "exception": false,
     "start_time": "2022-06-19T16:21:51.963838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.06it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.20it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.18it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.14it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code6.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39253d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:24:48.583806Z",
     "iopub.status.busy": "2022-06-19T16:24:48.582956Z",
     "iopub.status.idle": "2022-06-19T16:26:57.517657Z",
     "shell.execute_reply": "2022-06-19T16:26:57.517185Z",
     "shell.execute_reply.started": "2022-06-19T15:53:39.391398Z"
    },
    "papermill": {
     "duration": 129.001519,
     "end_time": "2022-06-19T16:26:57.517798",
     "exception": false,
     "start_time": "2022-06-19T16:24:48.516279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.62it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.51it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.43it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code7.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c082ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:26:58.224457Z",
     "iopub.status.busy": "2022-06-19T16:26:58.217237Z",
     "iopub.status.idle": "2022-06-19T16:29:26.593304Z",
     "shell.execute_reply": "2022-06-19T16:29:26.593754Z",
     "shell.execute_reply.started": "2022-06-19T15:56:18.945657Z"
    },
    "papermill": {
     "duration": 148.499838,
     "end_time": "2022-06-19T16:29:26.593931",
     "exception": false,
     "start_time": "2022-06-19T16:26:58.094093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:06<00:00,  2.29s/it]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.94it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.95it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.85it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python code8.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e23a3561",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:29:27.086850Z",
     "iopub.status.busy": "2022-06-19T16:29:27.086133Z",
     "iopub.status.idle": "2022-06-19T16:34:16.864853Z",
     "shell.execute_reply": "2022-06-19T16:34:16.864315Z",
     "shell.execute_reply.started": "2022-06-19T16:01:06.285488Z"
    },
    "papermill": {
     "duration": 289.861143,
     "end_time": "2022-06-19T16:34:16.864988",
     "exception": false,
     "start_time": "2022-06-19T16:29:27.003845",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36ex [00:00, 2034.86ex/s]\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.76it/s]loading configuration file ../input/deberta-v3-5folds/fold1/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-5folds/fold1\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-v3-5folds/fold1/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:14<00:00, 16.76it/s]All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ../input/deberta-v3-5folds/fold1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:22<00:00,  4.44s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.48it/s]loading configuration file ../input/deberta-v3-5folds/fold2/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-5folds/fold2\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-v3-5folds/fold2/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:12<00:00, 16.48it/s]All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ../input/deberta-v3-5folds/fold2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:21<00:00,  4.26s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.31it/s]loading configuration file ../input/deberta-v3-5folds/fold3/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-5folds/fold3\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-v3-5folds/fold3/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:10<00:00, 16.31it/s]All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ../input/deberta-v3-5folds/fold3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:21<00:00,  4.37s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.48it/s]loading configuration file ../input/deberta-v3-5folds/fold4/config.json\n",
      "Model config DebertaV2Config {\n",
      "  \"_name_or_path\": \"../input/deberta-v3-5folds/fold4\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaV2ForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-v3-5folds/fold4/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:18<00:00, 17.48it/s]All model checkpoint weights were used when initializing DebertaV2ForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaV2ForSequenceClassification were initialized from the model checkpoint at ../input/deberta-v3-5folds/fold4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaV2ForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.20s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 17.29it/s]Didn't find file ../input/bert-for-patent-5fold/fold0/added_tokens.json. We won't load it.\n",
      "loading file ../input/bert-for-patent-5fold/fold0/vocab.txt\n",
      "loading file ../input/bert-for-patent-5fold/fold0/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/bert-for-patent-5fold/fold0/special_tokens_map.json\n",
      "loading file ../input/bert-for-patent-5fold/fold0/tokenizer_config.json\n",
      "\n",
      "36ex [00:00, 3391.32ex/s]\n",
      "loading configuration file ../input/bert-for-patent-5fold/fold0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/bert-for-patent-5fold/fold0\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/bert-for-patent-5fold/fold0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/bert-for-patent-5fold/fold0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.35s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      " 80%|████████  | 4/5 [00:00<00:00, 30.37it/s]loading configuration file ../input/bert-for-patent-5fold/fold1/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/bert-for-patent-5fold/fold1\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/bert-for-patent-5fold/fold1/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:11<00:00, 30.37it/s]All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/bert-for-patent-5fold/fold1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.11s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      " 80%|████████  | 4/5 [00:00<00:00, 30.76it/s]loading configuration file ../input/bert-for-patent-5fold/fold2/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/bert-for-patent-5fold/fold2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/bert-for-patent-5fold/fold2/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/bert-for-patent-5fold/fold2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.01s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      " 60%|██████    | 3/5 [00:00<00:00, 28.55it/s]loading configuration file ../input/bert-for-patent-5fold/fold3/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/bert-for-patent-5fold/fold3\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/bert-for-patent-5fold/fold3/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:10<00:00, 28.55it/s]All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/bert-for-patent-5fold/fold3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.08s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      " 80%|████████  | 4/5 [00:00<00:00, 30.46it/s]loading configuration file ../input/bert-for-patent-5fold/fold4/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../input/bert-for-patent-5fold/fold4\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"regression\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 39859\n",
      "}\n",
      "\n",
      "loading weights file ../input/bert-for-patent-5fold/fold4/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ../input/bert-for-patent-5fold/fold4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:14<00:00,  2.95s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      " 80%|████████  | 4/5 [00:00<00:00, 30.71it/s]Didn't find file ../input/deberta-large-v1/fold0/added_tokens.json. We won't load it.\n",
      "loading file ../input/deberta-large-v1/fold0/vocab.json\n",
      "loading file ../input/deberta-large-v1/fold0/merges.txt\n",
      "loading file ../input/deberta-large-v1/fold0/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/deberta-large-v1/fold0/special_tokens_map.json\n",
      "loading file ../input/deberta-large-v1/fold0/tokenizer_config.json\n",
      "\n",
      "36ex [00:00, 3100.13ex/s]\n",
      "loading configuration file ../input/deberta-large-v1/fold0/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"../input/deberta-large-v1/fold0\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-large-v1/fold0/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:10<00:00, 30.71it/s]All model checkpoint weights were used when initializing DebertaForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaForSequenceClassification were initialized from the model checkpoint at ../input/deberta-large-v1/fold0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.46s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 15.85it/s]loading configuration file ../input/deberta-large-v1/fold1/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"../input/deberta-large-v1/fold1\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-large-v1/fold1/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:12<00:00, 15.85it/s]All model checkpoint weights were used when initializing DebertaForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaForSequenceClassification were initialized from the model checkpoint at ../input/deberta-large-v1/fold1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.58s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 15.78it/s]loading configuration file ../input/deberta-large-v1/fold2/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"../input/deberta-large-v1/fold2\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-large-v1/fold2/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:14<00:00, 15.78it/s]All model checkpoint weights were used when initializing DebertaForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaForSequenceClassification were initialized from the model checkpoint at ../input/deberta-large-v1/fold2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:16<00:00,  3.38s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.00it/s]loading configuration file ../input/deberta-large-v1/fold3/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"../input/deberta-large-v1/fold3\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-large-v1/fold3/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DebertaForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaForSequenceClassification were initialized from the model checkpoint at ../input/deberta-large-v1/fold3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.46s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 15.94it/s]loading configuration file ../input/deberta-large-v1/fold4/config.json\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"../input/deberta-large-v1/fold4\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/deberta-large-v1/fold4/pytorch_model.bin\n",
      "100%|██████████| 5/5 [00:10<00:00, 15.94it/s]All model checkpoint weights were used when initializing DebertaForSequenceClassification.\n",
      "\n",
      "All the weights of DebertaForSequenceClassification were initialized from the model checkpoint at ../input/deberta-large-v1/fold4.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DebertaForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.56s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 36\n",
      "  Batch size = 8\n",
      "100%|██████████| 5/5 [00:00<00:00, 16.11it/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 55.35ba/s]\n",
      "100%|██████████| 5/5 [00:00<00:00,  7.38it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python codep.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69afec5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:34:17.466588Z",
     "iopub.status.busy": "2022-06-19T16:34:17.465719Z",
     "iopub.status.idle": "2022-06-19T16:34:19.212804Z",
     "shell.execute_reply": "2022-06-19T16:34:19.212270Z",
     "shell.execute_reply.started": "2022-06-19T16:07:23.811148Z"
    },
    "papermill": {
     "duration": 1.847556,
     "end_time": "2022-06-19T16:34:19.212945",
     "exception": false,
     "start_time": "2022-06-19T16:34:17.365389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('python ens.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0dca8fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T16:34:19.462555Z",
     "iopub.status.busy": "2022-06-19T16:34:19.461622Z",
     "iopub.status.idle": "2022-06-19T16:34:19.538716Z",
     "shell.execute_reply": "2022-06-19T16:34:19.537505Z",
     "shell.execute_reply.started": "2022-06-19T16:07:33.211596Z"
    },
    "papermill": {
     "duration": 0.22887,
     "end_time": "2022-06-19T16:34:19.538903",
     "exception": false,
     "start_time": "2022-06-19T16:34:19.310033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in [1,4,5,6,7,8,'p']:\n",
    "    os.system('rm code{}.py'.format(i))\n",
    "    os.system('rm push{}.csv'.format(i))\n",
    "\n",
    "os.system('rm ens.py')\n",
    "os.system('rm -rf tmp_trainer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c9713",
   "metadata": {
    "papermill": {
     "duration": 0.175729,
     "end_time": "2022-06-19T16:34:19.912601",
     "exception": false,
     "start_time": "2022-06-19T16:34:19.736872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1136.797169,
   "end_time": "2022-06-19T16:34:23.613885",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-19T16:15:26.816716",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
